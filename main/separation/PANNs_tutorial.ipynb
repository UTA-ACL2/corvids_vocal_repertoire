{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9c2d89",
   "metadata": {},
   "source": [
    "# PANNs Post-Training Tutorial\n",
    "\n",
    "In this tutorial, we will go through the basic steps for post-training of PANNs (Pretrained Audio Neural Networks):\n",
    "\n",
    "1. Dataset preparation\n",
    "2. Data loading and preprocessing\n",
    "3. Model setup\n",
    "4. Training loop\n",
    "5. Validation and metrics\n",
    "6. Saving the best models\n",
    "\n",
    "Let‚Äôs start by importing the necessary libraries and setting up our environment!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afb0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from textgrid import TextGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from panns_inference.models import Cnn14_DecisionLevelMax  # From PANNs\n",
    "os.chdir('/data2/nitin/main/separation/audioset_tagging_cnn')\n",
    "\n",
    "from utils import config  # From PANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1c260",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Class and Label Extraction\n",
    "\n",
    "We start by defining the dataset class `TextGridDataset` which:\n",
    "- Loads audio files and corresponding TextGrid label files\n",
    "- Generates frame-wise labels\n",
    "- Computes class weights to handle class imbalance\n",
    "\n",
    "Below is the code for the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6622810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrid import TextGrid\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "def time_to_frame(time, sr, hop_size):\n",
    "    return int(time * sr // hop_size)\n",
    "\n",
    "def get_ground_truth(textgrid_file, audio_file, fixed_sample_points=64000, hop_size_samples=12800):\n",
    "    # Fixed label mapping (4 classes)\n",
    "    label_mapping = {\n",
    "        \"Crow\": 0,\n",
    "        \"Speech\": 1,\n",
    "        \"2+Crows\": 2,\n",
    "        \"Other\": 3\n",
    "    }\n",
    "\n",
    "    # Load TextGrid and audio\n",
    "    tg = TextGrid.fromFile(textgrid_file)\n",
    "    waveform, _ = librosa.load(audio_file, sr=32000, mono=True)\n",
    "\n",
    "    # Extract labeled intervals\n",
    "    intervals = []\n",
    "    for tier in tg.tiers:\n",
    "        if tier.__class__.__name__ == \"IntervalTier\":\n",
    "            for i in tier.intervals:\n",
    "                label = i.mark.strip()\n",
    "                if label and label in label_mapping:\n",
    "                    intervals.append((i.minTime, i.maxTime, label))\n",
    "                elif label and label not in label_mapping:\n",
    "                    print(f\"‚ö†Ô∏è Skipping unknown label '{label}' in {os.path.basename(textgrid_file)}\")\n",
    "\n",
    "    def create_labels(start_sample):\n",
    "        num_frames = fixed_sample_points // 320 + 1\n",
    "        clip_labels = np.zeros((num_frames, len(label_mapping)))\n",
    "        clip_barktype = np.full(num_frames, -1)\n",
    "\n",
    "        for s, e, label in intervals:\n",
    "            label_idx = label_mapping[label]\n",
    "            start_frame = time_to_frame(max(s, start_sample / 32000), 32000, 320) - (start_sample // 320)\n",
    "            end_frame = time_to_frame(min(e, (start_sample + fixed_sample_points) / 32000), 32000, 320) - (start_sample // 320)\n",
    "            clip_barktype[max(0, start_frame):end_frame] = label_idx\n",
    "            clip_labels[max(0, start_frame):end_frame, label_idx] = 1\n",
    "\n",
    "        return clip_labels, clip_barktype\n",
    "\n",
    "    # Segment audio\n",
    "    all_waveforms, all_labels, all_barktype, all_original_length = [], [], [], []\n",
    "    start_sample = 0\n",
    "    while start_sample + fixed_sample_points <= waveform.shape[0]:\n",
    "        clip = waveform[start_sample:start_sample + fixed_sample_points]\n",
    "        labels, barktype = create_labels(start_sample)\n",
    "        all_waveforms.append(clip)\n",
    "        all_labels.append(labels)\n",
    "        all_barktype.append(barktype)\n",
    "        all_original_length.append(fixed_sample_points)\n",
    "        start_sample += hop_size_samples\n",
    "\n",
    "    return all_waveforms, all_labels, all_barktype, all_original_length, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df07a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGridDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that loads .wav files and TextGrid label files.\n",
    "    It also computes class weights for balanced training.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_pairs, sample_rate=32000, hop_length=320):\n",
    "        self.waveforms, self.framewise_labels, self.barktype, self.original_length = [], [], [], []\n",
    "        self.hop_length = hop_length\n",
    "        self.label_mapping = None\n",
    "\n",
    "        for audio_path, textgrid_path in file_pairs:\n",
    "            waveform_list, gt_list, barktype_list, original_length_list, label_mapping = get_ground_truth(textgrid_path, audio_path)\n",
    "            if self.label_mapping is None:\n",
    "                self.label_mapping = label_mapping\n",
    "\n",
    "            self.waveforms.extend(waveform_list)\n",
    "            self.framewise_labels.extend(gt_list)\n",
    "            self.barktype.extend(barktype_list)\n",
    "            self.original_length.extend(original_length_list)\n",
    "\n",
    "        print(f\"Loaded {len(self.waveforms)} samples\")\n",
    "\n",
    "        # Validate consistent class count\n",
    "        num_classes = self.framewise_labels[0].shape[1]\n",
    "        for i, lbl in enumerate(self.framewise_labels):\n",
    "            if lbl.shape[1] != num_classes:\n",
    "                raise ValueError(f\"Sample {i} has {lbl.shape[1]} classes, expected {num_classes}\")\n",
    "\n",
    "        # Compute class weights\n",
    "        self.class_counts = torch.zeros(num_classes)\n",
    "        valid_frame_total = 0\n",
    "\n",
    "        for labels, orig_len in zip(self.framewise_labels, self.original_length):\n",
    "            labels_tensor = torch.from_numpy(labels).float()\n",
    "            valid_frames = int(orig_len // self.hop_length) + 1\n",
    "            self.class_counts += labels_tensor[:valid_frames].sum(dim=0)\n",
    "            valid_frame_total += valid_frames\n",
    "\n",
    "        self.class_counts[self.class_counts == 0] = 1e-6\n",
    "        self.class_weights = valid_frame_total / (num_classes * self.class_counts)\n",
    "        self.class_weights = self.class_weights / self.class_weights.sum() * num_classes\n",
    "\n",
    "        print(\"Class weights:\", self.class_weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.framewise_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform = torch.from_numpy(self.waveforms[idx]).float()\n",
    "        labels = torch.from_numpy(self.framewise_labels[idx]).float()\n",
    "        barktype = torch.from_numpy(self.barktype[idx]).float()\n",
    "        return waveform, labels, barktype, self.original_length[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6dcf54",
   "metadata": {},
   "source": [
    "## üêæ Model Building Blocks\n",
    "\n",
    "We define two key components of our model:\n",
    "\n",
    "- `ConvBlock`: Basic convolutional unit with BatchNorm, ReLU, and pooling.\n",
    "- `Cnn14_DecisionLevelMax`: Our 14-layer CNN with decision-level max pooling for audio event detection and classification.\n",
    "\n",
    "These modules will help you understand how to stack convolutional layers and modify pretrained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ffa922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from pytorch.models import SpecAugmentation, init_layer, init_bn\n",
    "from pytorch.pytorch_utils import interpolate, pad_framewise_output, do_mixup\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, x, pool_size=(2, 2), pool_type=\"avg\"):\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "\n",
    "        if pool_type == \"max\":\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == \"avg\":\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == \"avg+max\":\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size) + F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == \"no-downsample-avg\":\n",
    "            x = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        elif pool_type == \"no-pooling\":\n",
    "            pass  # no pooling\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid pooling type: {pool_type}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Cnn14_DecisionLevelMax(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate,\n",
    "        window_size,\n",
    "        hop_size,\n",
    "        mel_bins,\n",
    "        fmin,\n",
    "        fmax,\n",
    "        classes_num,\n",
    "        freeze_base,\n",
    "        pooling_type=\"avg\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolate_ratio = 32\n",
    "        self.freeze_base = freeze_base\n",
    "        self.pooling_type = pooling_type\n",
    "\n",
    "        # Spectrogram and logmel feature extraction\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=\"hann\",\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "        self.logmel_extractor = LogmelFilterBank(\n",
    "            sr=sample_rate,\n",
    "            n_fft=window_size,\n",
    "            n_mels=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            ref=1.0,\n",
    "            amin=1e-10,\n",
    "            top_db=None,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(\n",
    "            time_drop_width=64, time_stripes_num=2, freq_drop_width=8, freq_stripes_num=2\n",
    "        )\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "        self.conv_block1 = ConvBlock(1, 64)\n",
    "        self.conv_block2 = ConvBlock(64, 128)\n",
    "        self.conv_block3 = ConvBlock(128, 256)\n",
    "        self.conv_block4 = ConvBlock(256, 512)\n",
    "        self.conv_block5 = ConvBlock(512, 1024)\n",
    "        self.conv_block6 = ConvBlock(1024, 2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        input: (batch_size, data_length)\n",
    "        \"\"\"\n",
    "        x = self.spectrogram_extractor(input)\n",
    "        x = self.logmel_extractor(x)\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "            if mixup_lambda is not None:\n",
    "                x = do_mixup(x, mixup_lambda)\n",
    "\n",
    "        if self.pooling_type in [\"no-downsample-avg\", \"no-pooling\"]:\n",
    "            self.interpolate_ratio = 1\n",
    "\n",
    "        x = self.conv_block1(x, (2, 2), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "        x = self.conv_block2(x, (2, 2), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "        x = self.conv_block3(x, (2, 2), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "        x = self.conv_block4(x, (2, 2), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "        x = self.conv_block5(x, (2, 2), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "        x = self.conv_block6(x, (1, 1), self.pooling_type)\n",
    "        x = F.dropout(x, 0.2, self.training)\n",
    "\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x = F.max_pool1d(x, 3, 1, 1) + F.avg_pool1d(x, 3, 1, 1)\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        segmentwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        clipwise_output, _ = torch.max(segmentwise_output, dim=1)\n",
    "\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        return {\"framewise_output\": framewise_output, \"clipwise_output\": clipwise_output}\n",
    "\n",
    "    def load_and_modify_state_dict(self, state_dict, selected_classes_idx, num_additional_classes):\n",
    "        self.load_state_dict(state_dict)\n",
    "        original_fc_weight = self.fc_audioset.weight.data.clone()\n",
    "        original_fc_bias = self.fc_audioset.bias.data.clone()\n",
    "\n",
    "        selected_weight = original_fc_weight[selected_classes_idx, :]\n",
    "        selected_bias = original_fc_bias[selected_classes_idx]\n",
    "\n",
    "        new_fc = nn.Linear(2048, len(selected_classes_idx) + num_additional_classes)\n",
    "        new_fc.weight.data[:len(selected_classes_idx)] = selected_weight\n",
    "        new_fc.bias.data[:len(selected_classes_idx)] = selected_bias\n",
    "\n",
    "        if self.freeze_base:\n",
    "            for name, param in self.named_parameters():\n",
    "                if \"fc\" not in name:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        self.fc_audioset = new_fc\n",
    "        self.fc_audioset.weight.requires_grad = True\n",
    "        self.fc_audioset.bias.requires_grad = True\n",
    "\n",
    "        print(\"‚úÖ Pretrained weights loaded and final FC layer modified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d58254",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Training Function\n",
    "\n",
    "Now let‚Äôs define the function for training the model. We will:\n",
    "- Load the dataset\n",
    "- Instantiate the model\n",
    "- Define the loss and optimizer\n",
    "- Perform training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c033cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class FrameBceMaskedWeighted(nn.Module):\n",
    "    \"\"\"\n",
    "    BCE Loss for frame-wise predictions with class weights,\n",
    "    and increased penalty on false positives.\n",
    "    \"\"\"\n",
    "    def __init__(self, hop_length, pos_weight=1.0, neg_weight=1.0):\n",
    "        \"\"\"\n",
    "        pos_weight: weight for positive class (target=1)\n",
    "        neg_weight: weight for negative class (target=0), higher to penalize false positives\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hop_length = hop_length\n",
    "        self.pos_weight = pos_weight\n",
    "        self.neg_weight = neg_weight\n",
    "\n",
    "    def forward(self, pred, target, origin_length):\n",
    "        batch_ori_length = (origin_length // self.hop_length) + 1\n",
    "        loss_total = 0.0\n",
    "        for i in range(batch_ori_length.shape[0]):\n",
    "            ori_len = int(batch_ori_length[i])\n",
    "            p = pred.clamp(1e-7, 1.0)[i, :ori_len]  # predicted probs\n",
    "            t = target[i, :ori_len]                  # targets\n",
    "\n",
    "            # Create per-element weights:\n",
    "            # Weight positive targets by pos_weight, negatives by neg_weight\n",
    "            weights = torch.where(t == 1, \n",
    "                                  torch.tensor(self.pos_weight, device=t.device), \n",
    "                                  torch.tensor(self.neg_weight, device=t.device))\n",
    "\n",
    "            loss = F.binary_cross_entropy(p, t, weight=weights)\n",
    "            loss_total += loss\n",
    "\n",
    "        return loss_total / batch_ori_length.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a62b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, valid_dataset, model_type=\"Cnn14_DecisionLevelMax\", epochs=30, learning_rate=1e-3, device=\"cuda:0\"):\n",
    "    device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    model = eval(model_type)(32000, 1024, 320, 64, 50, 14000, config.classes_num, freeze_base=False, pooling_type=\"avg+max\")\n",
    "    pretrained_checkpoint_path = \"../Cnn14_mAP=0.431.pth\"\n",
    "    logging.info(\n",
    "        \"Load pretrained model from {}\".format(pretrained_checkpoint_path)\n",
    "    )\n",
    "    checkpoint = torch.load(\n",
    "        pretrained_checkpoint_path, map_location=device, weights_only=True\n",
    "    )\n",
    "    model.load_and_modify_state_dict(\n",
    "        checkpoint[\"model\"],\n",
    "        selected_classes_idx=[117, 0],\n",
    "        num_additional_classes=2,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, factor=0.1, min_lr=1e-7)\n",
    "    loss_fn = FrameBceMaskedWeighted(320, train_dataset.class_weights.to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for waveforms, labels, _, orig_len in train_loader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(waveforms, None)[\"framewise_output\"]\n",
    "            loss = loss_fn(outputs, labels, orig_len)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval() \n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        crow_correct, crow_total = 0, 0  # <-- For Crow label (index 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for waveforms, labels, _, orig_len in valid_loader:\n",
    "                waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                outputs = model(waveforms, None)[\"framewise_output\"]\n",
    "                loss = loss_fn(outputs, labels, orig_len)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=2)\n",
    "                targets = torch.argmax(labels, dim=2)\n",
    "\n",
    "                correct += (preds == targets).sum().item()\n",
    "                total += targets.numel()\n",
    "\n",
    "                # Compute Crow label accuracy (label index 0)\n",
    "                crow_mask = (targets == 0)  # Locations where Crow is ground truth\n",
    "                crow_correct += ((preds == targets) & crow_mask).sum().item()\n",
    "                crow_total += crow_mask.sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        val_acc = correct / total\n",
    "        crow_acc = crow_correct / crow_total if crow_total > 0 else 0.0\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"Crow Accuracy (class 0): {crow_acc:.4f}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a6a74",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Putting It All Together\n",
    "\n",
    "Now let‚Äôs load the dataset and start training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4f2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 audio-textgrid pairs.\n",
      "Loaded 14268 samples\n",
      "Class weights: tensor([0.0850, 3.3190, 0.2338, 0.3622])\n",
      "Loaded 3766 samples\n",
      "Class weights: tensor([0.3295, 2.5750, 0.6050, 0.4904])\n",
      "‚úÖ Pretrained weights loaded and final FC layer modified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011140/3156470390.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.pos_weight, device=t.device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.11714814295950492\n",
      "Validation Loss: 0.1759, Accuracy: 0.6774\n",
      "Crow Accuracy (class 0): 0.6879\n",
      "Model saved to ../fine_tuned_cnn14_clipwise.pth\n"
     ]
    }
   ],
   "source": [
    "file_folder = \"../fine_tuning_annotations\"\n",
    "all_files = []\n",
    "\n",
    "for f in os.listdir(file_folder):\n",
    "    if f.endswith(\".wav\"):\n",
    "        wav_path = os.path.join(file_folder, f)\n",
    "        tg_path = wav_path.replace(\".wav\", \".TextGrid\")\n",
    "        if os.path.exists(tg_path):\n",
    "            all_files.append((wav_path, tg_path))\n",
    "\n",
    "print(f\"Found {len(all_files)} audio-textgrid pairs.\")\n",
    "random.shuffle(all_files)\n",
    "train_size = int(0.8 * len(all_files))\n",
    "train_dataset = TextGridDataset(all_files[:train_size])\n",
    "valid_dataset = TextGridDataset(all_files[train_size:])\n",
    "\n",
    "model = train_model(train_dataset, valid_dataset, epochs=20)\n",
    "save_path = \"../fine_tuned_cnn14_framewise.pth\"\n",
    "torch.save({\"model\": model.state_dict()}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9148d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crowenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
